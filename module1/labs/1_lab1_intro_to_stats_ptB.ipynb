{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviewing Statistics and R, part B\n",
    "\n",
    "This is a continuation of your first lab for the Introduction to Statistical and Mathematical Foundations of Data Science course. \n",
    "You can refer to chapters 1 to 3 in [Intro to Statistics textbook](http://onlinestatbook.com/2/index.html) book for reference. \n",
    "\n",
    "We will begin with statistics mainly for univariate data analysis, \n",
    "covering some basic concepts like descriptive and inferential statistics and distributions.\n",
    "\n",
    "**We now continue with the final portion of the introductory lab**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferential Statistics\n",
    "\n",
    "Inferential statistic measures help us draw inferences about larger population from sample data. \n",
    "You rarely have access to full population datasets. \n",
    "\n",
    "Consider the following: \n",
    "The evidence linking cigarette smoking and lung disease is almost irrefutable. \n",
    "Based upon this information, what proportion of Americans have given up smoking? \n",
    "One way to answer this question would be to survey the entire population of the United States. \n",
    "It would be impossible from the standpoint of time and cost-effectiveness. \n",
    "Mathematicians have created methods of estimating population parameters from samples drawn from target populations that adequately represent the larger population. \n",
    "\n",
    "In inferential statistics we will be answering questions or testing hypotheses about populations based upon samples or prior data. \n",
    "Since parameters of populations are generally not available we must rely on sampling techniques to estimate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling Distributions\n",
    "\n",
    "The sampling distribution represents the distribution of a sample statistic considering all possible samples of a particular size that could come from a given population.  Put another way, a sampling distribution is the frequency distribution of a statistic \n",
    "over many random samples from a single population.  A _statistic_ is a numeric description of the sample, while a _parameter_ is a numeric description of the popluation.  We use sample _statistics_ to estimate population _parameters_.  \n",
    "We know that there will be some error involved when we attempt to relate those sample statistics to population parameters.  This error term can be estimated and is used in inference. \n",
    "\n",
    "\n",
    "For example, how do we know the average height of males in this country is 5'9\"? If we were to draw one hundred different samples of 10 males at random, we would find a certain amount of variability among the means and standard deviations of the different samples.  The _standard error of the mean_ is defined as the standard deviation of the sampling distribution of the mean. In other words, if you take many random samples from a population, the standard error of the mean is the standard deviation of the different sample means. \n",
    "\n",
    "When a researcher asserts something about a population using the information from a sample, this is called an inference.  The researcher makes this inference with the knowledge that there will be some descrepency and thus the possibility that their inference may be mistaken. A _Significance level_ reflects a \"tolerance level\" with the likelihood of making an incorrect inference.  Some researchers conclude that if the event would occur by chance 5% of time or less, then the event could be attributed to non-chance factors.  In other settings, researchers conclude that if the event would occur by chance 1% of time or less, then the event could be attributed to non-chance factors. These are the 0.05 and 0.01 significance levels. \n",
    "\n",
    "The problem/data domain and other factors drive the required significance levels for a particular statistical evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example \n",
    "\n",
    "In the case of male heights, we could choose at random 10 males and their average height would fall 69\" $\\pm$ 2.25\"(1.96) -- in other words, 64.59 to 73.41.  \n",
    "The sample standard deviation is multiplied by 1.96 since a z-score of $\\pm$1.96 would encompasses 95% of the normal distribution. \n",
    "Here if we are using 0.05 significance level we would theoretically be correct 95% of the time. \n",
    "Also, we would know that 5% of the samples we chose would have a mean height of greater than 73.41\" or less than 64.59.\"\n",
    "\n",
    "\n",
    "The $\\alpha = 0.05$ significance level is associated with a 95% confidence interval.  (The confidence level = 1 - $\\alpha$ and is usually expressed as a percentage.)  A confidence interval gives a range of values which is likely to include an unknown population parameter.  This interval is only exact when the population from which the sample is drawn follows a normal distribution. For large samples from other population distributions, the interval is approximately correct, due to the Central Limit Theorem.\n",
    "\n",
    "\n",
    "In order to perform inferential statistics or parametric tests of significance, \n",
    "we'll have to use sampling distributions. \n",
    "In order to create a sampling distribution we would need to draw all possible samples of size n from a given population. \n",
    "Once we have calculated the mean for each distribution the resulting distribution of these means would represent the sampling distribution of means.\n",
    "\n",
    "Sampling distributions have 3 characteristics:\n",
    "* The mean of the sampling distribution will not change with a change in sample size. If the mean from the sampling distribution of means is 20 when n=10, it will remain 20 whether you increase or decrease the size of the samples. Simply put, **the mean of the sampling distribution is equal to the mean of the population**.\n",
    "* As the sample size in the sampling distribution of means increases, the dispersion of sample means decreases. The larger the n, the more compact the distribution of sample means. As n increases, standard error of the mean decreases.\n",
    "* If the sampling distribution of means is taken from a normally distributed population, the sample means will also be bell shaped.\n",
    "\n",
    "Based on the above three characteristics, the **Central Limit Theorem** states:  \n",
    "If random samples of fixed size n are drawn from any population, as n becomes larger, \n",
    "the distribution of sample means approaches normality with the overall mean approaching $\\mu$  (i.e., the population mean).\n",
    "The standard error of the sample means is equal to\n",
    "\n",
    "$$\\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt n}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypotheses\n",
    "\n",
    "_Null Hypothesis ($H_0$):_ \n",
    "The null hypothesis specifies values for parameters. \n",
    "Generally referred to as the \"no significant difference\" hypothesis. \n",
    "A statement like \"this class is not significantly different from other statistics classes\" is example of a null hypothesis.  Note that most analyses are set up to `reject` or `not reject` the null hypothesis.\n",
    "\n",
    "_Alternate Hypothesis ($H_1$):_ \n",
    "The alternate hypothesis states that the population parameters are something other than the one hypothesized. \n",
    "A statement like \"this class is different from other statistics classes\" is example of an alternate hypothesis. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style='color:#A1BB6F'>Four main steps of hypothesis testing </h3>\n",
    "\n",
    "* State your null hypothesis ($H_0$) and alternative hypothesis ($H_A$)\n",
    "* Choose a significance level ($\\alpha$, alpha) of the test.\n",
    "* the test procedure, collect data, and calculate a p-value.\n",
    "* Use your p-value to make a decision:\n",
    "<br>\n",
    "<span style='color:red'> \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; If p-value $<$ $\\alpha$, reject $H_0$\n",
    "    <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; If p-value $>$ $\\alpha$, do not reject $H_0$\n",
    "</span>\n",
    "\n",
    "\n",
    "<span style='color:blue'>Relation between p-value and $\\alpha$</span>\n",
    "\n",
    "* If p-value < $\\alpha$, then your results are unlikely when the null is true. There is sufficient evidence to reject the null hypothesis and conclude the alternative. This is called a “statistically significant” result.\n",
    "\n",
    "* If p-value $> \\alpha$, then your results are likely when the null is true. There is insufficient evidence to reject the null hypothesis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Let’s find out the inference with which we can draw from the body dimensions (_bdims_) data set we are going to work on. \n",
    "The dataset contains body dimensions data from 247 men and 260 women. \n",
    "\n",
    "Let’s say, we want to check the significance of variable `sex` for hypothesis testing. \n",
    "Assume that males (`sex=1`) on average are heavier than the average population weight.\n",
    "\n",
    "To verify this assumption, let’s use a z-test and see if males are actually heavier than the overall population.\n",
    "\n",
    "$H_0$: There is no significant difference between the average weight of men and the average population weight\n",
    "\n",
    "$H_1$: There is a significant difference between the average weight of men and the average population weight\n",
    "\n",
    "\n",
    "Here we are checking if average male weight is different from average population weight. It is a 2-tailed test. For a two tailed test with a significance level of 0.05, the tail on both sides of the distribution is 0.025. (See this link: https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-what-are-the-differences-between-one-tailed-and-two-tailed-tests/.)  The critical z-scores associated with a two-tailed test at an alpha level of 0.05 are -1.96 and 1.96.  Using a z-score table, we can see that the areas under the standard normal curve that correspond to these critical z-scores are 0.0250 and 0.9750. \n",
    "\n",
    "The rejection regions are found to the left of the critical z value of -1.96 and to the right of the critical z value of 1.96."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download.file(\"http://www.openintro.org/stat/data/bdims.RData\", destfile = \"bdims.RData\")\n",
    "load(\"bdims.RData\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick peek into the first few rows of data...  \n",
    "Note that the weight (`wgt`) and `sex` are in the last three columns of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(bdims)\n",
    "summary(bdims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above result you see that every observation has 25 measurements. \n",
    "The variable names description can be found at https://www.openintro.org/book/statdata/?data=bdims. \n",
    "\n",
    "We will work with just two columns for now: weight in kg (`wgt`) and `sex` (1 indicates male, 0 indicates female).\n",
    "\n",
    "Let's go ahead and create two different data sets: one for men and one for women."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male <- subset(bdims, sex == 1)\n",
    "female <- subset(bdims, sex == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the test statistic\n",
    "\n",
    "\n",
    "#### calculating Z-score\n",
    "\n",
    "**A z-score represents the number of standard deviations a particular score is from the population mean.**  \n",
    "      \n",
    "The formula for the z-statistic is given as \n",
    "\n",
    "$$Z = \\frac{\\bar{X}-\\mu}{\\sigma_{\\bar{x}}}$$\n",
    "\n",
    "where,\n",
    "* $\\bar{x}$ = sample mean,\n",
    "* $\\mu$ = population mean and  \n",
    "* $\\sigma_{\\bar{x}}$ = standard deviation of the sampling distribution of $\\bar{x}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code computes some means, variances, z-scores, etc. of the bdims data set.  Recall from the Introduction notebook that variance is the average of the squared differences from the mean.  Standard deviation is the square root of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " sample_mean = mean(male$wgt)\n",
    " pop_mean = mean(bdims$wgt)\n",
    " pop_var = var(bdims$wgt)\n",
    " print(paste(\"sample mean : \",sample_mean))\n",
    " print(paste(\"population mean : \",pop_mean))\n",
    " print(paste(\"population variance : \",pop_var))\n",
    " zscore = (sample_mean - pop_mean) / (sqrt(pop_var)) #Standard Deviation\n",
    " print(paste(\"Z-score : \",zscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The z score is 0.67 after rounding it to 2 decimals. \n",
    "Now we need to work out the p-value.\n",
    "\n",
    "<h3 style='color:#685BEA'>Percentiles</h3>\n",
    "<br>\n",
    "\n",
    "* Percentile is the percentage of observations that fall below (i.e., to the left of) a given data point\n",
    "* Graphically, percentile is the area under the probability distribution curve to the left of that observation.\n",
    "\n",
    "<img src=\"../images/percentiles.png\">\n",
    "\n",
    "We refer to the standard normal distribution table to find out this percentage value. \n",
    "\n",
    "![Standard Normal Distribution Table](../images/normal-table-large.png)\n",
    "\n",
    "To read the table (i.e., to find our table area), we break our z-score into two parts 0.67 = 0.6 (_tenths_) + 0.07 (_hundredths_)\n",
    "\n",
    "The tenths component is used to find the appropriate row in the table.  The hundreths component is used for the column. You then find the cell in the table for that row and column, and this represents the % of the population that exhibits a smaller value than that which you obtained.\n",
    "\n",
    "Using the table, we can see that the table area is 0.7486.\n",
    "\n",
    "A p-value is, for one-tailed tests, a tail area or, for two-tailed tests, twice the tail area.  The tail area is the \"small\" part of the curve.  If z is positive, then the tail area is the area under the curve to the right of z.  In this case, tail area = 1 - table area.  If z is negative, then the tail area is the area under the curve to the left of z.  In this case, tail area = table area.  Using symmetry of the normal curve, we can write tail area = 1 - table area for $|z|$ = table area for $-|z|$.\n",
    "\n",
    "Again, the p-value is either the tail area (if the test is one-sided) or twice the tail area (if the test is two sided).\n",
    "\n",
    "We are dealing with a two-tailed hypothesis test with a stated alpha level of 0.05. Thus, again, the critical Z scores for this distribution are -1.96 and 1.96.  This mean we would reject the null hypothesis if the computed value of z from the sample (in this case z = .67) were less than -1.96 or greater than +1.96.  Because z = .67 is in the middle, we cannot reject the null hypothesis.\n",
    "\n",
    "The table area for z = .67 is .7486.  The tail area is 1 - .7486 = .2514.  Because the test is two-tailed, the p-value = 2(.2514) = .5028.\n",
    "\n",
    "The returned z score (0.67) does not lie within the rejection region, which is z<-1.96 or z>+1.96.  That is to say, .67 lies between -1.96 and 1.96, so there is insufficient evidence to reject the null hypothesis.\n",
    "\n",
    "Put another way <b>p-value (0.5028) >  $\\alpha$ (0.05),</b> so we cannot reject the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot the z-score and critical value for our hypothesis testing. To plot z-score values, data should be normally distributed. <span style=\"color:#ce7f5c\"> scale() </span> will normalize the data. The areas outside of red lines on both sides are critical regions. The critical value represented by green line is within critical region. So we fail to reject the null hypothesis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plain R\n",
    "x = scale(male$wgt)\n",
    "y <- dnorm(x, mean=mean(x), sd=(sqrt(var(x))))\n",
    "plot(x, y, type=\"n\", lwd=1)\n",
    "lines(x[order(x)], y[order(x)], pch=16)\n",
    "abline(v = 0.7486, col = \"green\", lwd = 2)\n",
    "abline(v = 1.96, col = \"red\", lwd = 2)\n",
    "abline(v = -1.96, col = \"red\", lwd = 2)\n",
    "text(x=c(-1.96, 0.7486, 1.96),y=0, labels=c(\"-1.96\",\"0.7486\", \"1.96\"))\n",
    "\n",
    "\n",
    "#ggplot \n",
    "df <- data.frame(x=x, y=y)\n",
    "library(ggplot2)\n",
    "p <- ggplot(df) + geom_line(aes(x=x,y=y)) \n",
    "p <- p + geom_vline(xintercept=1.96, color=\"red\")+ geom_vline(xintercept=-1.96, color=\"red\")+ geom_vline(xintercept=0.7486, color=\"green\")\n",
    "p <- p + geom_text(aes(x=1.96, label=1.96, y=0))\n",
    "p <- p + geom_text(aes(x=-1.96, label=-1.96, y=0))\n",
    "p <- p + geom_text(aes(x=0.7486, label=0.7486, y=0)) + theme_light()\n",
    "\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-Square analysis\n",
    "\n",
    "A non parametric test of significance is one that makes no assumption concerning the shape of the population distribution and is commonly referred to as a _distribution-free_ test of significance. \n",
    "Non parametric procedures are more suitable when data is categorical and for group comparison research.\n",
    "\n",
    "Chi square ($\\chi^2$) allows us to determine whether or not the proportion of observations \n",
    "within mutually exclusive categories differs significantly from the proportions expected by statistical chance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-Square Goodness of Fit\n",
    "\n",
    "This is the one variable (univariate) case and is used to determine whether significant differences occur within a single group. \n",
    "The null hypothesis can be tested by applying the formula below. \n",
    "\n",
    "$$\\chi^2 = \\sum\\frac{(f_o - f_e)^2}{f_e}$$\n",
    "\n",
    "where,\n",
    "\n",
    "* $f_o$ = the observed number in a given category, and\n",
    "* $f_e$ = the expected number in that category\n",
    "\n",
    "For example, a psychology department at a university has three emphasis area options available for incoming students: clinical psychology, educational psychology and counseling psychology. \n",
    "If students were to randomly select an area for study,\n",
    "probability would suggest that one-third would choose clinical psychology, \n",
    "one-third would choose educational psychology and one-third would choose counseling psychology. \n",
    "Let's say, there are 100 incoming students and 45 choose clinical psychology, \n",
    "30 choose educational psychology and 25 choose counseling psychology. \n",
    "\n",
    "We can construct a set of hypotheses to determine if differences in the number of students selecting each emphasis area are significantly different from out expectation, given our expectation is that each option will be selected by 1/3 of the students).\n",
    "We start with our Null Hypothesis and Alternative Hypothesis.\n",
    "\n",
    "$H_0$: No significant differences exist among students choosing academic options within psychology.\n",
    "\n",
    "$H_1$: Significant differences exist among students choosing academic options within psychology (relative to our expectation that each option will be selected by 1/3 of students).\n",
    "\n",
    "Test: $\\chi^2$\n",
    "\n",
    "$\\alpha$: 0.5\n",
    "\n",
    "Sampling Distribution: degrees of freedom: K-1 = 2      where K is the number of groups\n",
    "\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td>Cell 1 <br> Clicinal <br> Psychology </td>\n",
    "<td>Cell 2 <br> Educational <br> Psychology </td>\n",
    "<td>Cell 3 <br> Counseling <br> Psychology </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>$f_o$=45 <br> $f_e$=33.3 </td>\n",
    "<td>$f_o$=30 <br> $f_e$=33.3 </td>\n",
    "<td>$f_o$=25 <br> $f_e$=33.3 </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "$$\\chi^2 = \\frac{(45 - 33.3)^2}{33.3} + \\frac{(30 - 33.3)^2}{33.3} + \\frac{(25 - 33.3)^2}{33.3}$$\n",
    "\n",
    "$$ = 4.10 + 0.33 + 2.08$$\n",
    "\n",
    "$$ = 6.51$$\n",
    "\n",
    "Decision: \n",
    "Since the calculated $\\chi^2$ value of 6.51 exceeds the table value of 5.991 we would reject the null hypothesis. \n",
    "  * [See Chi Square table here](http://www.itl.nist.gov/div898/handbook/eda/section3/eda3674.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-Square Test of Independence\n",
    "\n",
    "Chi square test of independence of categorical variables is used when we describe differences between two or more groups in a two way table. \n",
    "It returns the probability for the computed chi-square distribution with the degree of freedom selected.\n",
    "\n",
    "Probability of 0: It indicates that both categorical variables are dependent.\n",
    "\n",
    "Probability of 1: It shows that both variables are independent.\n",
    "\n",
    "Probability less than 0.05: It indicates that the relationship between the variables is significant, given an alpha level of 0.05.\n",
    "\n",
    "We will use the built-in function `chisq.test()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(chisq.test)\n",
    "chisq.test(c(bdims$wgt,bdims$sex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chisq.test(c(bdims$wgt, bdims$hgt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the **p values** in both cases are < 0.05,\n",
    "we can infer that height and sex are highly significant variables and must be included in our final data modeling stage.\n",
    "We should perform chi squared tests on other variables in the dataset to see if they are \n",
    "significant and should be included in any data modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Correlation:\n",
    "\n",
    "Correlation determines the level of association between two variables. \n",
    "A scatter plot among the variables is one of the ways to find correlations between variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(bdims$wgt, bdims$hgt, xlab = 'weight', ylab = 'height')\n",
    "\n",
    "ggplot(bdims) + geom_point(aes(x=wgt, y=hgt), alpha=.3,size=4) + theme_light() + labs(x=\"weight\",y=\"height\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a positive correlation among `hgt` and `wgt` variables. \n",
    "R has a built-in function to measure the correlation. \n",
    "Let's use the `cor.test()` function to verify that height and weight variables are correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(cor.test)\n",
    "cor.test(bdims$wgt, bdims$hgt, method = 'pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the scatter plot suggests, the correlation function supports our assumption that height and weight are associated. \n",
    "The level of corelation is 0.717. \n",
    "\n",
    "You can perform tests on other variables in the dataset and similarly find associations among other variables. \n",
    "Variables that are highly correlated, e.g., 0.99 correlation, do not add much information to a predictive model. \n",
    "Therefore, when you have two indepedent variables that are highly correlated, you can usually drop one of these variables from your final model input design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This concludes the whirlwind review of descriptive and inferential statistics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
